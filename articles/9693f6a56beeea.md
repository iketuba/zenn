---
title: "transformersã®tutorialã‚’èª­ã‚“ã§ã¿ãŸ - part3"
emoji: "ğŸ”¥"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["python", "æ©Ÿæ¢°å­¦ç¿’", "åˆå¿ƒè€…", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"]
published: true
---
# ã¯ã˜ã‚ã«
https://huggingface.co/docs/transformers/training

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã¯å¤§ããªåˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚ è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€æœ€å…ˆç«¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã“ã¨ãªãä½¿ç”¨ã§ãã¾ã™ã€‚Transformersã‚’ä½¿ã†ã¨ã€å¹…åºƒã„ã‚¿ã‚¹ã‚¯å‘ã‘ã®ä½•åƒã‚‚ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ã‚¿ã‚¹ã‚¯ã«å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ã“ã‚Œã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ãŠã‚Šéå¸¸ã«æœ‰ç”¨ãªæ‰‹æ³•ã§ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨“ç·´ç”¨ã«æº–å‚™ã—ã¾ã™ã€‚

ã¾ãšã¯Yelpãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
```py
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]
```

æ¬¡ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆ©ç”¨ã—ã¦ã€paddingã‚„truncationãªã©ã®å‡¦ç†ã‚’è¡Œã„ã€é…åˆ—ã®é•·ã•ã‚’èª¿æ•´ã—ã¾ã™ã€‚(å‚è€ƒï¼špart2)
```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

tokenized_datasetsã‚’å‡ºåŠ›ã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
DatasetDict({
    train: Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 650000
    })
    test: Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 50000
    })
})

å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€æ‰€è¦æ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ã€‚
```py
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## Train
Transformersã¯ã€è¨“ç·´ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸTrainerã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚Transformersãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€å­¦ç¿’ã®ãƒ«ãƒ¼ãƒ—ã‚’æ‰‹å‹•ã§è¨˜è¿°ã—ãªãã¦ã‚‚ã€å­¦ç¿’ã‚’ç°¡å˜ã«å®Ÿè¡Œã§ãã¾ã™ã€‚

ã¾ãšã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€äºˆæƒ³ã•ã‚Œã‚‹ãƒ©ãƒ™ãƒ«ã®æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
æ¬¡ã«ã€èª¿æ•´å¯èƒ½ãªã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€TrainingArgumentsã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

å­¦ç¿’ã®çµæœã‚’ä¿å­˜ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã—ã¾ã™ã€‚
```py
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="/content/drive/MyDrive/tutorial/transformers")
```

### è©•ä¾¡é–¢æ•°
Trainerã¯ã€å­¦ç¿’ä¸­ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è‡ªå‹•çš„ã«è©•ä¾¡ã—ã¾ã›ã‚“ã€‚è©•ä¾¡é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ã€Trainerã«é–¢æ•°ã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚Datasetsã®load_metricé–¢æ•°ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€è©•ä¾¡é–¢æ•°ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
```py
import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")
```

å…ˆã»ã©è©•ä¾¡é–¢æ•°ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸmetricã«å¯¾ã—ã¦computeã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€äºˆæ¸¬ã®ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãªãŠã€computeã‚’ä½¿ã†å‰ã«ã€äºˆæ¸¬çµæœã‚’logitsã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
```py
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è©•ä¾¡é–¢æ•°ã®æ§˜å­ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ã€å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«è©•ä¾¡çµæœã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ã€Evaluation_strategyãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¾ã™ã€‚
```py
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

### Trainer
ãƒ¢ãƒ‡ãƒ«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãŠã‚ˆã³è©•ä¾¡é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦Trainerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
```py
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
```

train()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚
```py
trainer.train()
```