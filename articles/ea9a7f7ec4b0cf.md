---
title: "kaggle House Priceã«æŒ‘æˆ¦!"
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["python", "æ©Ÿæ¢°å­¦ç¿’", "åˆå¿ƒè€…", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"]
published: true
---
# åˆã‚ã«
ã“ã®è¨˜äº‹ã§ã¯ã€kaggleã®House Priceã‚³ãƒ³ãƒšã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ãã¾ã™ï¼ãªãŠã€ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ä¸‹ã•ã„ã€‚
https://www.kaggle.com/code/iketuba/house-price-2022-06-24

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
ã¾ãšã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ã‚’è¡Œã„ã¾ã™ã€‚
```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn import ensemble
from sklearn import linear_model
from sklearn import model_selection
from sklearn import preprocessing

pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)
```

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’è¡Œã„ã¾ã™ã€‚
```py
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€æå‡ºã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')
sample = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')
```

ãƒ‡ãƒ¼ã‚¿ã®å¤§ãã•ã‚’ç¢ºèªã—ã¾ã™ã€‚
```py
# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®ç¢ºèª
print('å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º:', train.shape)
print('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º:', test.shape)
```

ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚„å‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
```py
# ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã®ç¢ºèª
print(train.info())
print("-"*10)
print(test.info())
```

ã“ã“ã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã«ã¯æ¬ æå€¤ãŒå¤šãå«ã¾ã‚Œã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã®å‹ãŒobjectã¨ãªã£ã¦ã„ã‚‹å¤‰æ•°ãŒå¤šãã‚ã‚Šã€æ•°å€¤å¤‰æ•°ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚<br>
æ¬¡ã«ã€å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†ã‘ã¾ã™ã€‚
```py
# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã«åˆ†ã‘ã‚‹
train_x = train.drop(['SalePrice'], axis=1)
train_y = train['SalePrice']

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ç‰¹å¾´é‡ã®ã¿ãªã®ã§ãã®ã¾ã¾ã§è‰¯ã„
test_x = test.copy()
```

# ç‰¹å¾´é‡ã®ä½œæˆ
ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’ç¢ºèªã—ã¾ã™ã€‚
```py
#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’ç¢ºèªã™ã‚‹
print('è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤:\n', train_x.isnull().sum().sort_values(ascending=False))
print('è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤(å‰²åˆ):\n', 
      (train_x.isnull().sum() / len(train_x)).sort_values(ascending=False))
print("-"*10)
#ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’ç¢ºèªã™ã‚‹
print('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤:\n', test_x.isnull().sum().sort_values(ascending=False))
print('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤(å‰²åˆ):\n', 
      (test_x.isnull().sum() / len(test_x)).sort_values(ascending=False))
```

å¤‰æ•°Idã¯ç•ªå·ã‚’æŒ¯ã£ã¦ã„ã‚‹ã ã‘ã§ã‚ã‚Šã€å®¶ã®ä¾¡æ ¼ã«å½±éŸ¿ã‚’åŠã¼ã•ãªã„ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã®ã§ã€é™¤å¤–ã—ã¾ã™ã€‚
```py
# å¤‰æ•°Idã‚’é™¤å¤–ã™ã‚‹
train_x = train_x.drop(['Id'], axis=1)
test_x = test_x.drop(['Id'], axis=1)
```

æ¬¡ã«ã€æ¬ æå€¤ã®å‡¦ç†ã‚’ã—ã¾ã™ã€‚å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åŒã˜å‡¦ç†ã‚’ã™ã‚‹ãŸã‚ã€2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã™ã€‚ã“ã†ã—ãŸæ–¹ãŒã‚³ãƒ¼ãƒ‰ãŒçŸ­ãã¦æ¸ˆã¿ã¾ã™ã€‚
```py
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹
all_x = pd.concat([train_x, test_x])
```

æ¬ æå€¤ãŒå¤šã™ãã‚‹å¤‰æ•°ã¯å‰Šé™¤ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚ã“ã“ã§ã¯ã€æ¬ æå€¤ãŒåŠåˆ†ä»¥ä¸Šå ã‚ã‚‹å¤‰æ•°ã¯å‰Šé™¤ã—ã¾ã™ã€‚(åŠåˆ†ã«ã—ãŸã®ã¯ãªã‚“ã¨ãªãã§ã™)
```py
# æ¬ æå€¤ãŒåŠåˆ†ä»¥ä¸Šå ã‚ã‚‹ç‰¹å¾´é‡ã¯é™¤å¤–ã™ã‚‹
miss_cols = []

for col in all_x.columns:
    if all_x[col].isnull().sum()/len(all_x) > 0.5:
        miss_cols.append(col)
        
all_x = all_x.drop(miss_cols, axis=1)
```

å‰Šé™¤ã—ãªã‹ã£ãŸå¤‰æ•°ã®ä¸­ã§ã€æ¬ æã®ã‚ã‚‹å¤‰æ•°ã«ã¤ã„ã¦å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚æ•°å€¤å¤‰æ•°ã«ã¤ã„ã¦ã¯ä¸­å¤®å€¤ã§è£œå®Œã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚
```py
# æ•°å€¤å¤‰æ•°ã‹ã¤æ¬ æå€¤ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’ä¸­å¤®å€¤ã§è£œå®Œã™ã‚‹
# æ•°å€¤å¤‰æ•°ã‹ã¤æ¬ æå€¤ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’æŠœãå‡ºã™
num_miss_cols = []

for col in all_x.columns:
    if all_x[col].dtype in ['int64', 'float64'] and all_x[col].isnull().sum() > 0:
        num_miss_cols.append(col)
        
# æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œã™ã‚‹
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
imp_mean.fit(all_x[num_miss_cols])
all_x[num_miss_cols] = imp_mean.transform(all_x[num_miss_cols])
```

ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«ã¤ã„ã¦ã¯æœ€é »å€¤ã§è£œå®Œã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚
```py
# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‹ã¤æ¬ æå€¤ã‚’å«ã‚€åˆ—ã‚’æœ€é »å€¤ã§è£œå®Œã™ã‚‹
# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‹ã¤æ¬ æå€¤ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’æŠœãå‡ºã™
cat_miss_cols = []

for col in all_x.columns:
    if all_x[col].dtype == 'object' and all_x[col].isnull().sum() > 0:
        cat_miss_cols.append(col)
        
# æ¬ æå€¤ã‚’æœ€é »å€¤ã§è£œå®Œã™ã‚‹
from sklearn.impute import SimpleImputer
imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imp_most_frequent.fit(all_x[cat_miss_cols])
all_x[cat_miss_cols] = imp_most_frequent.transform(all_x[cat_miss_cols])
```

æ¬ æå€¤ã®å‡¦ç†ãŒã§ããŸã®ã§ã€çµåˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å†åˆ†å‰²ã—ã¾ã™ã€‚
```py
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å†åˆ†å‰²ã™ã‚‹
train_x = all_x.iloc[:train_x.shape[0], :]
test_x = all_x.iloc[train_x.shape[0]:, :]
```

æ¬¡ã«ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æ•°å€¤å¤‰æ•°ã«å¤‰æ›ã—ã¦ã„ãã¾ã™ã€‚ã“ã“ã§ã¯ã€ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚
```py
# è³ªçš„å¤‰æ•°ã‚’ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹
# è³ªçš„å¤‰æ•°ã‚’æŠœãå‡ºã™
cat_cols = []

for col in train_x.columns:
    if train_x[col].dtype == 'object':
        cat_cols.append(col)
        
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹
from sklearn.preprocessing import LabelEncoder
all_x = pd.concat([train_x, test_x])
for col in cat_cols:
    le = LabelEncoder()
    all_x[col] = le.fit_transform(all_x[col])

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å†åˆ†å‰²
train_x = all_x.iloc[:train_x.shape[0], :]
test_x = all_x.iloc[train_x.shape[0]:, :]
```

ã“ã‚Œã§ã€å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€å¾Œã«æ¨™æº–åŒ–ã‚’ã—ã¦ãŠãã¾ã™ã€‚
```py
# æ¨™æº–åŒ–
scaler = preprocessing.StandardScaler()
scaler.fit(train_x)
train_x = scaler.transform(train_x)
test_x = scaler.transform(test_x)
```

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è©•ä¾¡ã€æå‡º
åˆ†å‰²æ–¹æ³•ã‚’æŒ‡å®šã—ã¾ã™ã€‚
```py
# åˆ†å‰²æ–¹æ³•ã®æŒ‡å®š
kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=0)
```

äº¤å·®æ¤œè¨¼æ³•ã‚’ç”¨ã„ã¦ã€ç²¾åº¦ã‚’ç¢ºèªã—ã¾ã™ã€‚
```py
# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è©•ä¾¡(RandomForestRegressor)
rfr = ensemble.RandomForestRegressor()
rfr_results = model_selection.cross_validate(rfr, train_x, train_y, scoring='neg_root_mean_squared_error',cv=kf)
rfr_results['test_score'].mean()
```

ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚GridSearchCVã‚’ç”¨ã„ã¾ã—ãŸã€‚
```py
# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è©•ä¾¡ - ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ã‚Š(RandomForestRegressor)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 2, 5],
}

tune_rfr = model_selection.GridSearchCV(rfr, param_grid=param_grid, scoring = 'neg_root_mean_squared_error', cv = kf)
                                        
                                        
tune_rfr.fit(train_x, train_y)
print("æœ€ã‚‚ã‚ˆã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ", tune_rfr.best_params_)
print("æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤: ", tune_rfr.cv_results_['mean_test_score'][tune_rfr.best_index_])
```

ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµæœã€æœ€ã‚‚ç²¾åº¦ã®é«˜ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¾ã™ã€‚å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ã—ã¦ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚
```py
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ã™ã‚‹
tune_rfr.best_estimator_.fit(train_x, train_y)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã™ã‚‹
predict = tune_rfr.best_estimator_.predict(test_x)
```

æœ€å¾Œã«æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
```py
# æå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
submit_1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': predict})
submit_1.to_csv('submission.csv', index=False)
```

# æœ€å¾Œã«
ä»Šå›ã¯å„å¤‰æ•°ã«ã¤ã„ã¦ç´°ã‹ã„åˆ†æã‚’è¡Œã£ã¦ã„ãªã„ã®ã§ã€ã‚‚ã†å°‘ã—æ·±ãåˆ†æã‚’ã™ã‚‹è¨˜äº‹ã‚‚æ›¸ããŸã„ã¨æ€ã£ã¦ã¾ã™ï¼